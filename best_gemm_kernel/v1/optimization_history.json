[
  {
    "round": 0,
    "goal": "Baseline",
    "status": "Success",
    "time_ms": 7.2319999694824215,
    "ptxas_metrics": {
      "registers_used": 0.0,
      "shared_mem_bytes": 0.0,
      "spill_bytes": 0.0
    },
    "details": "Initial baseline measurement"
  },
  {
    "round": 1,
    "goal": "Implement blocked (shared-memory) tiling (e.g., 16x16 tiles) for GEMM: load tiles of A and B into shared memory per block, perform the tiled inner-product accumulation to maximize on-chip data reuse and reduce L2/global memory traffic.",
    "status": "Success (New Best)",
    "time_ms": 6.566092681884766,
    "ptxas_metrics": {
      "registers_used": 29.0,
      "shared_mem_bytes": 2176.0,
      "spill_bytes": 0.0
    },
    "details": "Performance improved from 7.232 ms to 6.566 ms.",
    "code": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#ifndef BLOCK_SIZE\n#define BLOCK_SIZE 16\n#endif\n\n// ------------------------------------------------------------------\n// KERNEL: gemm_kernel \n// ------------------------------------------------------------------\n__global__ void gemm_kernel(\n    const float* __restrict__ A,\n    const float* __restrict__ B,\n    float* __restrict__ C,\n    int N\n) {\n    // Tiled shared-memory GEMM kernel with bank-conflict padding\n    const int tx = threadIdx.x;\n    const int ty = threadIdx.y;\n    const int row = blockIdx.y * blockDim.y + ty;\n    const int col = blockIdx.x * blockDim.x + tx;\n\n    // Shared memory tiles with padding to reduce bank conflicts\n    __shared__ float As[BLOCK_SIZE][BLOCK_SIZE + 1];\n    __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE + 1];\n\n    float sum = 0.0f;\n\n    // Iterate over tiles of the K dimension\n    for (int t = 0; t < N; t += BLOCK_SIZE) {\n        // Global indices for this tile\n        const int tiledACol = t + tx;  // column into A for this tile\n        const int tiledBRow = t + ty;  // row into B for this tile\n\n        // Guarded loads into shared memory (coalesced)\n        if (row < N && tiledACol < N) {\n            As[ty][tx] = A[row * N + tiledACol];\n        } else {\n            As[ty][tx] = 0.0f;\n        }\n\n        if (tiledBRow < N && col < N) {\n            Bs[ty][tx] = B[tiledBRow * N + col];\n        } else {\n            Bs[ty][tx] = 0.0f;\n        }\n\n        // Ensure all data is loaded before computation\n        __syncthreads();\n\n        // Compute partial sum for this tile\n        if (row < N && col < N) {\n            #pragma unroll\n            for (int k = 0; k < BLOCK_SIZE; ++k) {\n                sum = fmaf(As[ty][k], Bs[k][tx], sum);\n            }\n        }\n\n        // Ensure all threads complete computation before loading the next tile\n        __syncthreads();\n    }\n\n    // Write the final result\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\n// ------------------------------------------------------------------\n// WRAPPER: gemm_cuda (\u8fd9\u662fPyTorch\u548cCUDA\u4e4b\u95f4\u7684\u6865\u6881)\n// ------------------------------------------------------------------\ntorch::Tensor gemm_cuda(torch::Tensor A, torch::Tensor B) {\n    \n    // --- \u8f93\u5165\u9a8c\u8bc1 ---\n    TORCH_CHECK(A.device().is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.device().is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.scalar_type() == torch::kFloat32, \"A must be float32\");\n    TORCH_CHECK(B.scalar_type() == torch::kFloat32, \"B must be float32\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D tensors\");\n    TORCH_CHECK(A.size(1) == B.size(0), \"Matrix dimensions mismatch\");\n    TORCH_CHECK(A.is_contiguous(), \"A must be contiguous\");\n    TORCH_CHECK(B.is_contiguous(), \"B must be contiguous\");\n\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n    TORCH_CHECK(M == N && K == N, \"This naive example assumes square N=M=K matrices\");\n    auto C = torch::zeros({M, N}, A.options());\n\n    // --- \u5185\u6838\u542f\u52a8\u914d\u7f6e ---\n    const int block_dim_x = BLOCK_SIZE;\n    const int block_dim_y = BLOCK_SIZE;\n    const int grid_dim_x = (N + block_dim_x - 1) / block_dim_x;\n    const int grid_dim_y = (N + block_dim_y - 1) / block_dim_y;\n    dim3 blocks(grid_dim_x, grid_dim_y);\n    dim3 threads(block_dim_x, block_dim_y);\n\n    // --- \u542f\u52a8\u5185\u6838 ---\n    gemm_kernel<<<blocks, threads>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        N\n    );\n\n    // --- \u9519\u8bef\u68c0\u67e5 ---\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(\"CUDA error in gemm_kernel: \" + std::string(cudaGetErrorString(err)));\n    }\n    return C;\n}"
  },
  {
    "round": 2,
    "goal": "Increase arithmetic intensity and ALU utilization by using a larger tile and register blocking (e.g., move from 16x16 to 32x32 tiling with per-thread/register blocking and K-loop unrolling) so each global load yields more fused multiply-adds on-chip and better saturates the SM compute units.",
    "status": "Success (New Best)",
    "time_ms": 4.3726848602294925,
    "ptxas_metrics": {
      "registers_used": 32.0,
      "shared_mem_bytes": 8448.0,
      "spill_bytes": 0.0
    },
    "details": "Performance improved from 6.566 ms to 4.373 ms.",
    "code": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#ifndef BLOCK_SIZE\n#define BLOCK_SIZE 32\n#endif\n\n#ifndef THREAD_TILE_M\n#define THREAD_TILE_M 2\n#endif\n\n#ifndef THREAD_TILE_N\n#define THREAD_TILE_N 2\n#endif\n\n// ------------------------------------------------------------------\n// KERNEL: gemm_kernel \n// ------------------------------------------------------------------\n__global__ void gemm_kernel(\n    const float* __restrict__ A,\n    const float* __restrict__ B,\n    float* __restrict__ C,\n    int N\n) {\n    // Register-blocked, tiled shared-memory GEMM kernel\n    const int tx = threadIdx.x;\n    const int ty = threadIdx.y;\n\n    const int blockRowBase = blockIdx.y * BLOCK_SIZE;\n    const int blockColBase = blockIdx.x * BLOCK_SIZE;\n\n    // Shared memory tiles with padding to reduce bank conflicts\n    __shared__ float As[BLOCK_SIZE][BLOCK_SIZE + 1];\n    __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE + 1];\n\n    // Each compute thread will handle a 2x2 patch inside a 32x32 tile.\n    // Map compute threads to 2x2 patches; other threads participate in loads and sync.\n    const int rowTile0 = ty * THREAD_TILE_M;\n    const int colTile0 = tx * THREAD_TILE_N;\n\n    const int row0 = blockRowBase + rowTile0 + 0;\n    const int row1 = row0 + 1;\n    const int col0 = blockColBase + colTile0 + 0;\n    const int col1 = col0 + 1;\n\n    float c00 = 0.0f, c01 = 0.0f, c10 = 0.0f, c11 = 0.0f;\n\n    // Iterate over tiles of the K dimension\n    for (int t = 0; t < N; t += BLOCK_SIZE) {\n        // Cooperative loads: with BLOCK_SIZE=32, each thread loads one element\n        const int aRow = blockRowBase + ty;\n        const int aCol = t + tx;\n        const int bRow = t + ty;\n        const int bCol = blockColBase + tx;\n\n        // Guarded loads into shared memory\n        As[ty][tx] = (aRow < N && aCol < N) ? A[aRow * N + aCol] : 0.0f;\n        Bs[ty][tx] = (bRow < N && bCol < N) ? B[bRow * N + bCol] : 0.0f;\n\n        __syncthreads();\n\n        // Only a subset of threads perform the 2x2 register-blocked compute to produce a 32x32 tile\n        if (ty < (BLOCK_SIZE / THREAD_TILE_M) && tx < (BLOCK_SIZE / THREAD_TILE_N)) {\n            #pragma unroll\n            for (int k = 0; k < BLOCK_SIZE; ++k) {\n                float a0 = As[rowTile0 + 0][k];\n                float a1 = As[rowTile0 + 1][k];\n                float b0 = Bs[k][colTile0 + 0];\n                float b1 = Bs[k][colTile0 + 1];\n\n                c00 = fmaf(a0, b0, c00);\n                c01 = fmaf(a0, b1, c01);\n                c10 = fmaf(a1, b0, c10);\n                c11 = fmaf(a1, b1, c11);\n            }\n        }\n\n        __syncthreads();\n    }\n\n    // Write the final 2x2 results from compute threads\n    if (ty < (BLOCK_SIZE / THREAD_TILE_M) && tx < (BLOCK_SIZE / THREAD_TILE_N)) {\n        if (row0 < N && col0 < N) C[row0 * N + col0] = c00;\n        if (row0 < N && col1 < N) C[row0 * N + col1] = c01;\n        if (row1 < N && col0 < N) C[row1 * N + col0] = c10;\n        if (row1 < N && col1 < N) C[row1 * N + col1] = c11;\n    }\n}\n\n// ------------------------------------------------------------------\n// WRAPPER: gemm_cuda (\u8fd9\u662fPyTorch\u548cCUDA\u4e4b\u95f4\u7684\u6865\u6881)\n// ------------------------------------------------------------------\ntorch::Tensor gemm_cuda(torch::Tensor A, torch::Tensor B) {\n    \n    // --- \u8f93\u5165\u9a8c\u8bc1 ---\n    TORCH_CHECK(A.device().is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.device().is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.scalar_type() == torch::kFloat32, \"A must be float32\");\n    TORCH_CHECK(B.scalar_type() == torch::kFloat32, \"B must be float32\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D tensors\");\n    TORCH_CHECK(A.size(1) == B.size(0), \"Matrix dimensions mismatch\");\n    TORCH_CHECK(A.is_contiguous(), \"A must be contiguous\");\n    TORCH_CHECK(B.is_contiguous(), \"B must be contiguous\");\n\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n    TORCH_CHECK(M == N && K == N, \"This naive example assumes square N=M=K matrices\");\n    auto C = torch::zeros({M, N}, A.options());\n\n    // --- \u5185\u6838\u542f\u52a8\u914d\u7f6e ---\n    const int block_dim_x = BLOCK_SIZE;\n    const int block_dim_y = BLOCK_SIZE;\n    const int grid_dim_x = (N + block_dim_x - 1) / block_dim_x;\n    const int grid_dim_y = (N + block_dim_y - 1) / block_dim_y;\n    dim3 blocks(grid_dim_x, grid_dim_y);\n    dim3 threads(block_dim_x, block_dim_y);\n\n    // --- \u542f\u52a8\u5185\u6838 ---\n    gemm_kernel<<<blocks, threads>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        N\n    );\n\n    // --- \u9519\u8bef\u68c0\u67e5 ---\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(\"CUDA error in gemm_kernel: \" + std::string(cudaGetErrorString(err)));\n    }\n    return C;\n}"
  },
  {
    "round": 3,
    "goal": "Move to mixed\u2011precision Tensor Core GEMM (use FP16 inputs with FP32 accumulates via WMMA or equivalent), implementing tiled WMMA fragments with double\u2011buffered loads to saturate Tensor/Core ALUs and massively increase compute throughput.",
    "status": "Failed (Correctness)",
    "time_ms": null,
    "ptxas_metrics": {
      "registers_used": 40.0,
      "shared_mem_bytes": 10240.0,
      "spill_bytes": 0.0
    },
    "details": "New kernel is INCORRECT.",
    "code": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <mma.h>\n#include <cuda_fp16.h>\n\n#ifndef BLOCK_SIZE\n#define BLOCK_SIZE 32\n#endif\n\n#ifndef THREAD_TILE_M\n#define THREAD_TILE_M 2\n#endif\n\n#ifndef THREAD_TILE_N\n#define THREAD_TILE_N 2\n#endif\n\n// WMMA constants\n#ifndef WMMA_M\n#define WMMA_M 16\n#endif\n#ifndef WMMA_N\n#define WMMA_N 16\n#endif\n#ifndef WMMA_K\n#define WMMA_K 16\n#endif\n\n// Double buffering stages\n#ifndef STAGES\n#define STAGES 2\n#endif\n\n// Shared memory padding (bank-conflict mitigation)\n#ifndef PAD_A\n#define PAD_A 8\n#endif\n#ifndef PAD_B\n#define PAD_B 8\n#endif\n\n// ------------------------------------------------------------------\n// KERNEL: gemm_kernel \n// ------------------------------------------------------------------\n__global__ void gemm_kernel(\n    const float* __restrict__ A,\n    const float* __restrict__ B,\n    float* __restrict__ C,\n    int N\n) {\n#if __CUDA_ARCH__ >= 700\n    // WMMA mixed-precision path: convert FP32 inputs to half in shared memory and\n    // use Tensor Cores to compute; accumulate in FP32 and store to FP32 C.\n    using namespace nvcuda;\n\n    // Block origin in output matrix\n    const int blockRowBase = blockIdx.y * BLOCK_SIZE;\n    const int blockColBase = blockIdx.x * BLOCK_SIZE;\n\n    // Warp and lane identification\n    const int laneId = threadIdx.x & 31;        // 0..31\n    const int warpId = threadIdx.y;             // since blockDim.x == 32\n    const int tid = threadIdx.y * blockDim.x + threadIdx.x; // linear thread id in block\n\n    // Warp tiling for a 32x32 block: 2x2 WMMA tiles per block => 4 warps used for compute\n    constexpr int WARPS_M = BLOCK_SIZE / WMMA_M;   // 32/16 = 2\n    constexpr int WARPS_N = BLOCK_SIZE / WMMA_N;   // 32/16 = 2\n    constexpr int WARPS_PER_BLOCK = WARPS_M * WARPS_N; // 4\n\n    const int warp_m = warpId / WARPS_N;  // 0..1\n    const int warp_n = warpId % WARPS_N;  // 0..1\n\n    // Shared memory: double-buffered tiles with padding\n    __shared__ half As[STAGES][BLOCK_SIZE][WMMA_K + PAD_A];     // A tile: 32 x (16 + pad)\n    __shared__ half Bs[STAGES][WMMA_K + PAD_B][BLOCK_SIZE];     // B tile: (16 + pad) x 32\n    __shared__ float Csmem[WARPS_PER_BLOCK][WMMA_M][WMMA_N];    // for boundary-safe stores\n\n    // Accumulator fragment for each warp\n    wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, float> cFrag;\n    if (warpId < WARPS_PER_BLOCK) {\n        wmma::fill_fragment(cFrag, 0.0f);\n    }\n\n    // Preload first K tile (k0 = 0) into stage 0\n    int s = 0;\n    {\n        // Cooperative loads for A (BLOCK_SIZE x WMMA_K) and B (WMMA_K x BLOCK_SIZE)\n        const int totalA = BLOCK_SIZE * WMMA_K;     // 512 elements\n        const int totalB = WMMA_K * BLOCK_SIZE;     // 512 elements\n\n        if (tid < totalA) {\n            int a_r = tid / WMMA_K;\n            int a_c = tid % WMMA_K;\n            int gRow = blockRowBase + a_r;\n            int gCol = 0 + a_c;\n            half val = (gRow < N && gCol < N) ? __float2half(A[gRow * N + gCol]) : __float2half(0.0f);\n            As[s][a_r][a_c] = val;\n        }\n        if (tid < totalB) {\n            int b_r = tid / BLOCK_SIZE;\n            int b_c = tid % BLOCK_SIZE;\n            int gRow = 0 + b_r;\n            int gCol = blockColBase + b_c;\n            half val = (gRow < N && gCol < N) ? __float2half(B[gRow * N + gCol]) : __float2half(0.0f);\n            Bs[s][b_r][b_c] = val;\n        }\n    }\n    __syncthreads();\n\n    // K-loop with simple ping-pong buffering in shared memory\n    for (int k0 = 0; k0 < N; k0 += WMMA_K) {\n        // Load A and B fragments from shared memory and compute\n        if (warpId < WARPS_PER_BLOCK) {\n            const half* aPtr = &As[s][warp_m * WMMA_M][0];\n            const half* bPtr = &Bs[s][0][warp_n * WMMA_N];\n\n            constexpr int strideA = WMMA_K + PAD_A; // row-major leading dimension (columns per row)\n            constexpr int strideB = WMMA_K + PAD_B; // col-major leading dimension (rows per column)\n\n            wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> aFrag;\n            wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major> bFrag;\n\n            wmma::load_matrix_sync(aFrag, aPtr, strideA);\n            wmma::load_matrix_sync(bFrag, bPtr, strideB);\n\n            wmma::mma_sync(cFrag, aFrag, bFrag, cFrag);\n        }\n\n        // Preload next K tile into the other stage buffer\n        int next_k = k0 + WMMA_K;\n        int ns = s ^ 1;\n        if (next_k < N) {\n            const int totalA = BLOCK_SIZE * WMMA_K;     // 512\n            const int totalB = WMMA_K * BLOCK_SIZE;     // 512\n\n            if (tid < totalA) {\n                int a_r = tid / WMMA_K;\n                int a_c = tid % WMMA_K;\n                int gRow = blockRowBase + a_r;\n                int gCol = next_k + a_c;\n                half val = (gRow < N && gCol < N) ? __float2half(A[gRow * N + gCol]) : __float2half(0.0f);\n                As[ns][a_r][a_c] = val;\n            }\n            if (tid < totalB) {\n                int b_r = tid / BLOCK_SIZE;\n                int b_c = tid % BLOCK_SIZE;\n                int gRow = next_k + b_r;\n                int gCol = blockColBase + b_c;\n                half val = (gRow < N && gCol < N) ? __float2half(B[gRow * N + gCol]) : __float2half(0.0f);\n                Bs[ns][b_r][b_c] = val;\n            }\n        }\n        __syncthreads();\n        s = ns;\n    }\n\n    // Store results for each compute warp\n    if (warpId < WARPS_PER_BLOCK) {\n        const int cRow = blockRowBase + warp_m * WMMA_M;\n        const int cCol = blockColBase + warp_n * WMMA_N;\n\n        // Interior tile: direct store to global memory\n        if ((cRow + WMMA_M) <= N && (cCol + WMMA_N) <= N) {\n            wmma::store_matrix_sync(&C[cRow * N + cCol], cFrag, N, wmma::mem_row_major);\n        } else {\n            // Boundary tile: store to shared memory then scatter with bounds checks\n            wmma::store_matrix_sync(&Csmem[warpId][0][0], cFrag, WMMA_N, wmma::mem_row_major);\n            __syncthreads();\n            for (int i = 0; i < WMMA_M; ++i) {\n                int gRow = cRow + i;\n                if (gRow >= N) continue;\n                for (int j = 0; j < WMMA_N; ++j) {\n                    int gCol = cCol + j;\n                    if (gCol >= N) continue;\n                    C[gRow * N + gCol] = Csmem[warpId][i][j];\n                }\n            }\n        }\n    }\n#else\n    // Fallback to original FP32 tiled kernel for older architectures\n    const int tx = threadIdx.x;\n    const int ty = threadIdx.y;\n\n    const int blockRowBase = blockIdx.y * BLOCK_SIZE;\n    const int blockColBase = blockIdx.x * BLOCK_SIZE;\n\n    // Shared memory tiles with padding to reduce bank conflicts\n    __shared__ float As_fallback[BLOCK_SIZE][BLOCK_SIZE + 1];\n    __shared__ float Bs_fallback[BLOCK_SIZE][BLOCK_SIZE + 1];\n\n    // Each compute thread will handle a 2x2 patch inside a 32x32 tile.\n    // Map compute threads to 2x2 patches; other threads participate in loads and sync.\n    const int rowTile0 = ty * THREAD_TILE_M;\n    const int colTile0 = tx * THREAD_TILE_N;\n\n    const int row0 = blockRowBase + rowTile0 + 0;\n    const int row1 = row0 + 1;\n    const int col0 = blockColBase + colTile0 + 0;\n    const int col1 = col0 + 1;\n\n    float c00 = 0.0f, c01 = 0.0f, c10 = 0.0f, c11 = 0.0f;\n\n    // Iterate over tiles of the K dimension\n    for (int t = 0; t < N; t += BLOCK_SIZE) {\n        // Cooperative loads: with BLOCK_SIZE=32, each thread loads one element\n        const int aRow = blockRowBase + ty;\n        const int aCol = t + tx;\n        const int bRow = t + ty;\n        const int bCol = blockColBase + tx;\n\n        // Guarded loads into shared memory\n        As_fallback[ty][tx] = (aRow < N && aCol < N) ? A[aRow * N + aCol] : 0.0f;\n        Bs_fallback[ty][tx] = (bRow < N && bCol < N) ? B[bRow * N + bCol] : 0.0f;\n\n        __syncthreads();\n\n        // Only a subset of threads perform the 2x2 register-blocked compute to produce a 32x32 tile\n        if (ty < (BLOCK_SIZE / THREAD_TILE_M) && tx < (BLOCK_SIZE / THREAD_TILE_N)) {\n            #pragma unroll\n            for (int k = 0; k < BLOCK_SIZE; ++k) {\n                float a0 = As_fallback[rowTile0 + 0][k];\n                float a1 = As_fallback[rowTile0 + 1][k];\n                float b0 = Bs_fallback[k][colTile0 + 0];\n                float b1 = Bs_fallback[k][colTile0 + 1];\n\n                c00 = fmaf(a0, b0, c00);\n                c01 = fmaf(a0, b1, c01);\n                c10 = fmaf(a1, b0, c10);\n                c11 = fmaf(a1, b1, c11);\n            }\n        }\n\n        __syncthreads();\n    }\n\n    // Write the final 2x2 results from compute threads\n    if (ty < (BLOCK_SIZE / THREAD_TILE_M) && tx < (BLOCK_SIZE / THREAD_TILE_N)) {\n        if (row0 < N && col0 < N) C[row0 * N + col0] = c00;\n        if (row0 < N && col1 < N) C[row0 * N + col1] = c01;\n        if (row1 < N && col0 < N) C[row1 * N + col0] = c10;\n        if (row1 < N && col1 < N) C[row1 * N + col1] = c11;\n    }\n#endif\n}\n\n// ------------------------------------------------------------------\n// WRAPPER: gemm_cuda (\u8fd9\u662fPyTorch\u548cCUDA\u4e4b\u95f4\u7684\u6865\u6881)\n// ------------------------------------------------------------------\ntorch::Tensor gemm_cuda(torch::Tensor A, torch::Tensor B) {\n    \n    // --- \u8f93\u5165\u9a8c\u8bc1 ---\n    TORCH_CHECK(A.device().is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.device().is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.scalar_type() == torch::kFloat32, \"A must be float32\");\n    TORCH_CHECK(B.scalar_type() == torch::kFloat32, \"B must be float32\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D tensors\");\n    TORCH_CHECK(A.size(1) == B.size(0), \"Matrix dimensions mismatch\");\n    TORCH_CHECK(A.is_contiguous(), \"A must be contiguous\");\n    TORCH_CHECK(B.is_contiguous(), \"B must be contiguous\");\n\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n    TORCH_CHECK(M == N && K == N, \"This naive example assumes square N=M=K matrices\");\n    auto C = torch::zeros({M, N}, A.options());\n\n    // --- \u5185\u6838\u542f\u52a8\u914d\u7f6e ---\n    const int block_dim_x = BLOCK_SIZE;\n    const int block_dim_y = BLOCK_SIZE;\n    const int grid_dim_x = (N + block_dim_x - 1) / block_dim_x;\n    const int grid_dim_y = (N + block_dim_y - 1) / block_dim_y;\n    dim3 blocks(grid_dim_x, grid_dim_y);\n    dim3 threads(block_dim_x, block_dim_y);\n\n    // --- \u542f\u52a8\u5185\u6838 ---\n    gemm_kernel<<<blocks, threads>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        N\n    );\n\n    // --- \u9519\u8bef\u68c0\u67e5 ---\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(\"CUDA error in gemm_kernel: \" + std::string(cudaGetErrorString(err)));\n    }\n    return C;\n}"
  },
  {
    "round": 4,
    "goal": "Increase per\u2011thread work to raise arithmetic intensity and better saturate ALUs by enlarging the thread\u2011level register blocking (e.g., change THREAD_TILE_M and THREAD_TILE_N from 2 to 4 to make each thread compute a 4x4 patch), unroll the K loop accordingly and hoist/prefetch data into registers so each global/shared load yields many more FMAs.",
    "status": "Failed (Performance Regression)",
    "time_ms": 7.078604888916016,
    "ptxas_metrics": {
      "registers_used": 56.0,
      "shared_mem_bytes": 8448.0,
      "spill_bytes": 0.0
    },
    "details": "New time 7.079 ms is not better than best time 4.373 ms.",
    "code": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#ifndef BLOCK_SIZE\n#define BLOCK_SIZE 32\n#endif\n\n#ifndef THREAD_TILE_M\n#define THREAD_TILE_M 4\n#endif\n\n#ifndef THREAD_TILE_N\n#define THREAD_TILE_N 4\n#endif\n\nstatic_assert(BLOCK_SIZE % THREAD_TILE_M == 0, \"BLOCK_SIZE must be divisible by THREAD_TILE_M\");\nstatic_assert(BLOCK_SIZE % THREAD_TILE_N == 0, \"BLOCK_SIZE must be divisible by THREAD_TILE_N\");\n\n// ------------------------------------------------------------------\n// KERNEL: gemm_kernel \n// ------------------------------------------------------------------\n__global__ void gemm_kernel(\n    const float* __restrict__ A,\n    const float* __restrict__ B,\n    float* __restrict__ C,\n    int N\n) {\n    // Register-blocked, tiled shared-memory GEMM kernel\n    // This version performs 4x4 per-thread register blocking within a 32x32 tile.\n    // All 1024 threads cooperatively load tiles into shared memory; a gated subset\n    // of threads performs the register-blocked compute to produce the 32x32 output tile.\n    const int tx = threadIdx.x;\n    const int ty = threadIdx.y;\n\n    const int blockRowBase = blockIdx.y * BLOCK_SIZE;\n    const int blockColBase = blockIdx.x * BLOCK_SIZE;\n\n    // Shared memory tiles with +1 padding to reduce bank conflicts\n    __shared__ float As[BLOCK_SIZE][BLOCK_SIZE + 1];\n    __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE + 1];\n\n    // Map threads to 4x4 output patches inside the 32x32 tile\n    const int rowTile0 = ty * THREAD_TILE_M;\n    const int colTile0 = tx * THREAD_TILE_N;\n\n    // Absolute output indices for the 4 rows and 4 columns produced by a compute thread\n    const int row0 = blockRowBase + rowTile0 + 0;\n    const int row1 = row0 + 1;\n    const int row2 = row0 + 2;\n    const int row3 = row0 + 3;\n\n    const int col0 = blockColBase + colTile0 + 0;\n    const int col1 = col0 + 1;\n    const int col2 = col0 + 2;\n    const int col3 = col0 + 3;\n\n    // Number of threads that perform compute (others assist with loads and sync)\n    const int computeThreadsY = BLOCK_SIZE / THREAD_TILE_M;\n    const int computeThreadsX = BLOCK_SIZE / THREAD_TILE_N;\n\n    // 16 accumulators per compute thread for a 4x4 output patch\n    float c00 = 0.0f, c01 = 0.0f, c02 = 0.0f, c03 = 0.0f;\n    float c10 = 0.0f, c11 = 0.0f, c12 = 0.0f, c13 = 0.0f;\n    float c20 = 0.0f, c21 = 0.0f, c22 = 0.0f, c23 = 0.0f;\n    float c30 = 0.0f, c31 = 0.0f, c32 = 0.0f, c33 = 0.0f;\n\n    // Iterate over tiles of the K dimension\n    for (int t = 0; t < N; t += BLOCK_SIZE) {\n        // Cooperative loads: with BLOCK_SIZE=32, each thread loads one element\n        const int aRow = blockRowBase + ty;\n        const int aCol = t + tx;\n        const int bRow = t + ty;\n        const int bCol = blockColBase + tx;\n\n        // Guarded loads into shared memory\n        As[ty][tx] = (aRow < N && aCol < N) ? A[aRow * N + aCol] : 0.0f;\n        Bs[ty][tx] = (bRow < N && bCol < N) ? B[bRow * N + bCol] : 0.0f;\n\n        __syncthreads();\n\n        // Only a subset of threads perform the 4x4 register-blocked compute\n        if (ty < computeThreadsY && tx < computeThreadsX) {\n            #pragma unroll\n            for (int k = 0; k < BLOCK_SIZE; ++k) {\n                // Hoist shared memory values into registers for this K step\n                float a0 = As[rowTile0 + 0][k];\n                float a1 = As[rowTile0 + 1][k];\n                float a2 = As[rowTile0 + 2][k];\n                float a3 = As[rowTile0 + 3][k];\n\n                float b0 = Bs[k][colTile0 + 0];\n                float b1 = Bs[k][colTile0 + 1];\n                float b2 = Bs[k][colTile0 + 2];\n                float b3 = Bs[k][colTile0 + 3];\n\n                // 16 FMAs for the 4x4 patch\n                c00 = fmaf(a0, b0, c00); c01 = fmaf(a0, b1, c01); c02 = fmaf(a0, b2, c02); c03 = fmaf(a0, b3, c03);\n                c10 = fmaf(a1, b0, c10); c11 = fmaf(a1, b1, c11); c12 = fmaf(a1, b2, c12); c13 = fmaf(a1, b3, c13);\n                c20 = fmaf(a2, b0, c20); c21 = fmaf(a2, b1, c21); c22 = fmaf(a2, b2, c22); c23 = fmaf(a2, b3, c23);\n                c30 = fmaf(a3, b0, c30); c31 = fmaf(a3, b1, c31); c32 = fmaf(a3, b2, c32); c33 = fmaf(a3, b3, c33);\n            }\n        }\n\n        __syncthreads();\n    }\n\n    // Write the final 4x4 results from compute threads with bounds checks\n    if (ty < computeThreadsY && tx < computeThreadsX) {\n        if (row0 < N && col0 < N) C[row0 * N + col0] = c00;\n        if (row0 < N && col1 < N) C[row0 * N + col1] = c01;\n        if (row0 < N && col2 < N) C[row0 * N + col2] = c02;\n        if (row0 < N && col3 < N) C[row0 * N + col3] = c03;\n\n        if (row1 < N && col0 < N) C[row1 * N + col0] = c10;\n        if (row1 < N && col1 < N) C[row1 * N + col1] = c11;\n        if (row1 < N && col2 < N) C[row1 * N + col2] = c12;\n        if (row1 < N && col3 < N) C[row1 * N + col3] = c13;\n\n        if (row2 < N && col0 < N) C[row2 * N + col0] = c20;\n        if (row2 < N && col1 < N) C[row2 * N + col1] = c21;\n        if (row2 < N && col2 < N) C[row2 * N + col2] = c22;\n        if (row2 < N && col3 < N) C[row2 * N + col3] = c23;\n\n        if (row3 < N && col0 < N) C[row3 * N + col0] = c30;\n        if (row3 < N && col1 < N) C[row3 * N + col1] = c31;\n        if (row3 < N && col2 < N) C[row3 * N + col2] = c32;\n        if (row3 < N && col3 < N) C[row3 * N + col3] = c33;\n    }\n}\n\n// ------------------------------------------------------------------\n// WRAPPER: gemm_cuda (\u8fd9\u662fPyTorch\u548cCUDA\u4e4b\u95f4\u7684\u6865\u6881)\n// ------------------------------------------------------------------\ntorch::Tensor gemm_cuda(torch::Tensor A, torch::Tensor B) {\n    \n    // --- \u8f93\u5165\u9a8c\u8bc1 ---\n    TORCH_CHECK(A.device().is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.device().is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.scalar_type() == torch::kFloat32, \"A must be float32\");\n    TORCH_CHECK(B.scalar_type() == torch::kFloat32, \"B must be float32\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D tensors\");\n    TORCH_CHECK(A.size(1) == B.size(0), \"Matrix dimensions mismatch\");\n    TORCH_CHECK(A.is_contiguous(), \"A must be contiguous\");\n    TORCH_CHECK(B.is_contiguous(), \"B must be contiguous\");\n\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n    TORCH_CHECK(M == N && K == N, \"This naive example assumes square N=M=K matrices\");\n    auto C = torch::zeros({M, N}, A.options());\n\n    // --- \u5185\u6838\u542f\u52a8\u914d\u7f6e ---\n    const int block_dim_x = BLOCK_SIZE;\n    const int block_dim_y = BLOCK_SIZE;\n    const int grid_dim_x = (N + block_dim_x - 1) / block_dim_x;\n    const int grid_dim_y = (N + block_dim_y - 1) / block_dim_y;\n    dim3 blocks(grid_dim_x, grid_dim_y);\n    dim3 threads(block_dim_x, block_dim_y);\n\n    // --- \u542f\u52a8\u5185\u6838 ---\n    gemm_kernel<<<blocks, threads>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        N\n    );\n\n    // --- \u9519\u8bef\u68c0\u67e5 ---\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(\"CUDA error in gemm_kernel: \" + std::string(cudaGetErrorString(err)));\n    }\n    return C;\n}"
  },
  {
    "round": 5,
    "goal": "Implement software\u2011pipelined double\u2011buffered shared\u2011memory tiling with K\u2011loop unrolling and register prefetching (prefetch next tile into a second shared\u2011memory buffer and into registers while computing on the current tile) to increase instruction\u2011level parallelism and overlap loads with FMAs, boosting ALU utilization without increasing thread count per block.",
    "status": "Success (New Best)",
    "time_ms": 4.336640167236328,
    "ptxas_metrics": {
      "registers_used": 32.0,
      "shared_mem_bytes": 16896.0,
      "spill_bytes": 0.0
    },
    "details": "Performance improved from 4.373 ms to 4.337 ms.",
    "code": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#ifndef BLOCK_SIZE\n#define BLOCK_SIZE 32\n#endif\n\n#ifndef THREAD_TILE_M\n#define THREAD_TILE_M 2\n#endif\n\n#ifndef THREAD_TILE_N\n#define THREAD_TILE_N 2\n#endif\n\n#ifndef UNROLL_K\n#define UNROLL_K 4\n#endif\n\n// ------------------------------------------------------------------\n// KERNEL: gemm_kernel \n// ------------------------------------------------------------------\n__global__ void gemm_kernel(\n    const float* __restrict__ A,\n    const float* __restrict__ B,\n    float* __restrict__ C,\n    int N\n) {\n    // Register-blocked, tiled shared-memory GEMM kernel with software pipelining.\n    // Pipeline: preload tile 0 into buf0, compute on buf (cur), while prefetching next tile to registers.\n    // After compute, commit prefetched tile to the other buffer (next), sync, ping-pong buffers, repeat.\n\n    const int tx = threadIdx.x;\n    const int ty = threadIdx.y;\n\n    const int blockRowBase = blockIdx.y * BLOCK_SIZE;\n    const int blockColBase = blockIdx.x * BLOCK_SIZE;\n\n    // Double-buffered shared memory tiles with padding to reduce bank conflicts\n    __shared__ float As_buf0[BLOCK_SIZE][BLOCK_SIZE + 1];\n    __shared__ float Bs_buf0[BLOCK_SIZE][BLOCK_SIZE + 1];\n    __shared__ float As_buf1[BLOCK_SIZE][BLOCK_SIZE + 1];\n    __shared__ float Bs_buf1[BLOCK_SIZE][BLOCK_SIZE + 1];\n\n    // Mapping of compute threads to 2x2 patches inside a 32x32 tile\n    const int rowTile0 = ty * THREAD_TILE_M;\n    const int colTile0 = tx * THREAD_TILE_N;\n\n    const int row0 = blockRowBase + rowTile0 + 0;\n    const int row1 = row0 + 1;\n    const int col0 = blockColBase + colTile0 + 0;\n    const int col1 = col0 + 1;\n\n    const bool isComputeThread =\n        (ty < (BLOCK_SIZE / THREAD_TILE_M)) && (tx < (BLOCK_SIZE / THREAD_TILE_N));\n\n    // Global indices reused across iterations\n    const int aRow = blockRowBase + ty;\n    const int bCol = blockColBase + tx;\n\n    // Accumulators for the 2x2 per-thread patch\n    float c00 = 0.0f, c01 = 0.0f, c10 = 0.0f, c11 = 0.0f;\n\n    // --------------------------\n    // Stage 0: preload first K-tile (t = 0) into buf0\n    // --------------------------\n    int aCol0 = 0 + tx;\n    int bRow0 = 0 + ty;\n\n    As_buf0[ty][tx] = (aRow < N && aCol0 < N) ? A[aRow * N + aCol0] : 0.0f;\n    Bs_buf0[ty][tx] = (bRow0 < N && bCol < N) ? B[bRow0 * N + bCol] : 0.0f;\n\n    __syncthreads();\n\n    int buf = 0; // 0 => using buf0 as current, 1 => using buf1 as current\n\n    // --------------------------\n    // Main loop over K dimension with ping-pong buffering\n    // --------------------------\n    for (int t = 0; t < N; t += BLOCK_SIZE) {\n        // Select current and next shared-memory buffers\n        float (*curAs)[BLOCK_SIZE + 1]  = (buf == 0) ? As_buf0 : As_buf1;\n        float (*curBs)[BLOCK_SIZE + 1]  = (buf == 0) ? Bs_buf0 : Bs_buf1;\n        float (*nextAs)[BLOCK_SIZE + 1] = (buf == 0) ? As_buf1 : As_buf0;\n        float (*nextBs)[BLOCK_SIZE + 1] = (buf == 0) ? Bs_buf1 : Bs_buf0;\n\n        const int tNext = t + BLOCK_SIZE;\n        const bool hasNext = (tNext < N);\n\n        // Prefetch the next K tile from global memory into registers to overlap with compute\n        float aPref = 0.0f;\n        float bPref = 0.0f;\n        if (hasNext) {\n            const int aColNext = tNext + tx;\n            const int bRowNext = tNext + ty;\n            aPref = (aRow < N && aColNext < N) ? A[aRow * N + aColNext] : 0.0f;\n            bPref = (bRowNext < N && bCol < N) ? B[bRowNext * N + bCol] : 0.0f;\n        }\n\n        // Compute on the current shared-memory tile with conservative K-loop unrolling\n        if (isComputeThread) {\n            #pragma unroll\n            for (int k = 0; k < BLOCK_SIZE; k += UNROLL_K) {\n                #pragma unroll\n                for (int u = 0; u < UNROLL_K; ++u) {\n                    float a0 = curAs[rowTile0 + 0][k + u];\n                    float a1 = curAs[rowTile0 + 1][k + u];\n                    float b0 = curBs[k + u][colTile0 + 0];\n                    float b1 = curBs[k + u][colTile0 + 1];\n\n                    c00 = fmaf(a0, b0, c00);\n                    c01 = fmaf(a0, b1, c01);\n                    c10 = fmaf(a1, b0, c10);\n                    c11 = fmaf(a1, b1, c11);\n                }\n            }\n        }\n\n        // Commit the prefetched next tile into the next shared-memory buffers\n        if (hasNext) {\n            // Keep boundary guards consistent; write zeros when out-of-bounds\n            nextAs[ty][tx] = (aRow < N) ? aPref : 0.0f;\n            nextBs[ty][tx] = (bCol < N) ? bPref : 0.0f;\n        }\n\n        // Ensure the next tile is fully committed before it is used in the next iteration\n        __syncthreads();\n\n        // Ping-pong between buffers\n        buf ^= 1;\n    }\n\n    // Write the final 2x2 results from compute threads with bounds checks\n    if (isComputeThread) {\n        if (row0 < N && col0 < N) C[row0 * N + col0] = c00;\n        if (row0 < N && col1 < N) C[row0 * N + col1] = c01;\n        if (row1 < N && col0 < N) C[row1 * N + col0] = c10;\n        if (row1 < N && col1 < N) C[row1 * N + col1] = c11;\n    }\n}\n\n// ------------------------------------------------------------------\n// WRAPPER: gemm_cuda (\u8fd9\u662fPyTorch\u548cCUDA\u4e4b\u95f4\u7684\u6865\u6881)\n// ------------------------------------------------------------------\ntorch::Tensor gemm_cuda(torch::Tensor A, torch::Tensor B) {\n    \n    // --- \u8f93\u5165\u9a8c\u8bc1 ---\n    TORCH_CHECK(A.device().is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.device().is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.scalar_type() == torch::kFloat32, \"A must be float32\");\n    TORCH_CHECK(B.scalar_type() == torch::kFloat32, \"B must be float32\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D tensors\");\n    TORCH_CHECK(A.size(1) == B.size(0), \"Matrix dimensions mismatch\");\n    TORCH_CHECK(A.is_contiguous(), \"A must be contiguous\");\n    TORCH_CHECK(B.is_contiguous(), \"B must be contiguous\");\n\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n    TORCH_CHECK(M == N && K == N, \"This naive example assumes square N=M=K matrices\");\n    auto C = torch::zeros({M, N}, A.options());\n\n    // --- \u5185\u6838\u542f\u52a8\u914d\u7f6e ---\n    const int block_dim_x = BLOCK_SIZE;\n    const int block_dim_y = BLOCK_SIZE;\n    const int grid_dim_x = (N + block_dim_x - 1) / block_dim_x;\n    const int grid_dim_y = (N + block_dim_y - 1) / block_dim_y;\n    dim3 blocks(grid_dim_x, grid_dim_y);\n    dim3 threads(block_dim_x, block_dim_y);\n\n    // --- \u542f\u52a8\u5185\u6838 ---\n    gemm_kernel<<<blocks, threads>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        N\n    );\n\n    // --- \u9519\u8bef\u68c0\u67e5 ---\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(\"CUDA error in gemm_kernel: \" + std::string(cudaGetErrorString(err)));\n    }\n    return C;\n}"
  }
]